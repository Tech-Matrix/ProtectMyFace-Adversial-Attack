# -*- coding: utf-8 -*-
"""CopyAdvImg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dwrE4_RuN47W_YagWoo0kUzbn-vW5wHH
"""

pip install anvil-uplink

import urllib
import PIL

import os

import anvil.server
import anvil.media

anvil.server.connect("5C2A4XIEELEGHBQ44ADGY4UH-MLWJOGTTALTI7TBF")

!pip install git+https://github.com/VibNY/keras-vggface.git

import tensorflow as tf
import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rcParams['figure.figsize'] = (8, 8)
mpl.rcParams['axes.grid'] = False

loss_object = tf.keras.losses.CategoricalCrossentropy()

!pip install keras_applications

!pip install keras

import keras_vggface

from keras_vggface.vggface import VGGFace
from keras_vggface.utils import preprocess_input
from keras_vggface.utils import decode_predictions
from keras_vggface.utils import index

pretrained_model=VGGFace(model='resnet50')
decode_pred=decode_predictions

!pip install mtcnn

from numpy import expand_dims
from matplotlib import pyplot
from PIL import Image
from numpy import asarray
from mtcnn.mtcnn import MTCNN

"""from google.colab import drive
drive.mount('/content/gdrive')"""

s="obj.jpg"
l=[]

def extract_face(filename, required_size=(224, 224)):
  print("In extract face")
	# load image from file
  pixels = pyplot.imread(filename)
	# create the detector, using default weights
  detector = MTCNN()
	# detect faces in the image
  results = detector.detect_faces(pixels)
	# extract the bounding box from the first face
  x1, y1, width, height = results[0]['box']
  x2, y2 = x1 + width, y1 + height
	# extract the face
  face = pixels[y1:y2, x1:x2]
	# resize pixels to the model size
  image = Image.fromarray(face)
  image = image.resize(required_size)
  face_array = asarray(image)
  return face_array

def preprocess(image):
  print("in preprocess")
  image = tf.cast(image, tf.float32)
  image = tf.image.resize(image, (224, 224))
  image = tf.keras.applications.mobilenet_v2.preprocess_input(image)
  image = image[None, ...]
  return image

i=0

def imgtf(link):
  global i
  print("In imgtf")
  print(link)
  image_path = tf.keras.utils.get_file(s+str(i),link)
  i=i+1
  image_raw = tf.io.read_file(image_path)
  image = tf.image.decode_image(image_raw)
  image=preprocess(image)
  #print(type(image))
  return image

def get_image(link):
  urllib.request.urlretrieve(str(link), "sample.png")
  img = PIL.Image.open("sample.png")
  img.save("img.jpg", format="JPEG")

url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Mark_Ruffalo_%2836201774756%29_%28cropped%29.jpg/1200px-Mark_Ruffalo_%2836201774756%29_%28cropped%29.jpg'
get_image(url)

def create_adversarial_pattern(input_image, input_label):
  print("in adv pat")
  with tf.GradientTape() as tape:
    tape.watch(input_image)
    prediction = pretrained_model(input_image)
    loss = loss_object(input_label, prediction)

  # Get the gradients of the loss w.r.t to the input image.
  gradient = tape.gradient(loss, input_image)
  # Get the sign of the gradients to create the perturbation
  signed_grad = tf.sign(gradient)
  return signed_grad

def display_images(adv_x, description,i):
  print("In display")
  plt.figure() 
  plt.imshow(adv_x[0]*0.5); 
  plt.savefig('saved_figure'+str(i)+'.jpg')
  img='saved_figure'+str(i)+'.jpg'
  pixels = extract_face(img)
  # convert one face into samples
  pixels = pixels.astype('float32')
  samples = expand_dims(pixels, axis=0)
  # prepare the face for the model, e.g. center pixels
  samples = preprocess_input(samples, version=2)
  # create a vggface model
  model = VGGFace(model='resnet50')
  # perform prediction
  yhat = model.predict(samples)
  #print("yhat",yhat)
  # convert prediction into names
  results = decode_predictions(yhat)
  #print(results)
  desc=results[0][0]
  print(desc)
  name=str(desc[0])
  #print(name)
  #print(desc[1])
  #print("In function")
  conf=float(desc[1])
  plt.title('{} \n {} : {:.2f}% Confidence'.format(description,
                                                   name, conf*100))
  name='face'+str(i)+'.jpg'
  plt.savefig(name)
  

  r=anvil.media.from_file(name)
  return r

 # !cp <name> /content/drive/<Face_images

def final(perturbations,image):
  global l
  l=[]
  print("should be empty list",l)
  print("in final")
  epsilons = [0, 0.03, 0.07, 0.1]
  descriptions = [('Epsilon = {:0.3f}'.format(eps) if eps else 'Input')
                  for eps in epsilons]

  for i, eps in enumerate(epsilons):
    adv_x = image + eps*perturbations
    adv_x = tf.clip_by_value(adv_x, -1, 1)
    temp=display_images(adv_x, descriptions[i],i)
    l.append(temp)
  return l

def main(img,link):
  pixels = extract_face(img)
  # convert one face into samples
  pixels = pixels.astype('float32')
  samples = expand_dims(pixels, axis=0)
  # prepare the face for the model, e.g. center pixels
  samples = preprocess_input(samples, version=2)
  # create a vggface model
  model = VGGFace(model='resnet50')
  yhat = model.predict(samples)
  print(yhat)
  index_pic=index(yhat)
  label = tf.one_hot(index_pic, yhat.shape[-1])
  print(label)
  label = tf.reshape(label, (1, yhat.shape[-1]))
  print(label)
  perturbations = create_adversarial_pattern(imgtf(link), label)
  print("DONE")
  l=final(perturbations,imgtf(link))
  print(l)
  cmd = "rm *.jpg"
  returned_value = os.system(cmd)
  cmd = "rm *.png"
  returned_value = os.system(cmd)

  return l
  #plt.imshow(perturbations[0] * 0.5 + 0.5);  # To change [-1, 1] to [0,1]

@anvil.server.callable
def send_adv_img(link):
  get_image(link)
  faces=main('img.jpg',link)
  return faces

"""get_image('https://images.tribuneindia.com/cms/gall_content/2019/4/2019_4$largeimg15_Monday_2019_083425749.jpg')
main('img.jpg','https://images.tribuneindia.com/cms/gall_content/2019/4/2019_4$largeimg15_Monday_2019_083425749.jpg')"""

anvil.server.wait_forever()

"""# load the photo and extract the face
pixels = extract_face('rdj.jpg')
# convert one face into samples
pixels = pixels.astype('float32')
samples = expand_dims(pixels, axis=0)
# prepare the face for the model, e.g. center pixels
samples = preprocess_input(samples, version=2)
# create a vggface model
model = VGGFace(model='resnet50')
# perform prediction
yhat = model.predict(samples)
print("yhat",yhat)
# convert prediction into names
results = decode_predictions(yhat)
# display most likely results
for result in results[0]:
	print('%s: %.3f%%' % (result[0], result[1]*100))

def extract_face_org(filename, required_size=(224, 224)):
	# load image from file
	pixels = pyplot.imread(filename)
	# create the detector, using default weights
	detector = MTCNN()
	# detect faces in the image
	results = detector.detect_faces(pixels)
	# extract the bounding box from the first face
	x1, y1, width, height = results[0]['box']
	x2, y2 = x1 + width, y1 + height
	# extract the face
	face = pixels[y1:y2, x1:x2]
	# resize pixels to the model size
	image = Image.fromarray(face)
	image = image.resize(required_size)
	return image
"""